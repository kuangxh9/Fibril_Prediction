{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j1H-ec3xCMyD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "azHsvuSpCM9q"
   },
   "outputs": [],
   "source": [
    "# Adjust display settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = 'data/results_LSS_updated'\n",
    "\n",
    "# Initialize lists to store sequences and the last column values\n",
    "sequences = []\n",
    "last_column_values = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 2:  # Ensure the line has at least two parts\n",
    "            sequence = parts[1]  # Sequence (2nd column)\n",
    "            last_value = int(parts[-1])  # Last column (number)\n",
    "            \n",
    "            sequences.append(sequence[:-1])\n",
    "            last_column_values.append(last_value)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'sequence': sequences,\n",
    "    'Status': last_column_values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequence  Status\n",
      "0  FEFKEKFF       0\n",
      "1  FKFEEKFF       0\n",
      "2  EFKEKFFF       0\n",
      "3  FFFKEKFE       0\n",
      "4  KFFKEFFE       0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(88, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "print(df.head(5))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jHujxKl4CNCP",
    "outputId": "e0117b4c-9995-4281-c0ce-da698c77c0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequence  Status fibril_state\n",
      "0  FEFKEKFF       0   non-fibril\n",
      "1  FKFEEKFF       0   non-fibril\n",
      "2  EFKEKFFF       0   non-fibril\n",
      "3  FFFKEKFE       0   non-fibril\n",
      "4  KFFKEFFE       0   non-fibril\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df[df['Status'].isin([0, 2, 3, 4])].copy() \n",
    "\n",
    "# Add 'fibril_state' column based on 'Status' values\n",
    "df_filtered['fibril_state'] = df_filtered['Status'].apply(lambda x: 'fibril'\n",
    " if x in [2, 3, 4] else 'non-fibril')\n",
    "\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "print(df_filtered.head())\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "Mt322S3IFIlF"
   },
   "outputs": [],
   "source": [
    "#@title Plot Training History\n",
    "def plot_training_history(history, fold):\n",
    "  fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "  axs[0].plot(history.history['accuracy'])\n",
    "  axs[0].plot(history.history['val_accuracy'])\n",
    "  if isinstance(fold, int):\n",
    "    axs[0].set_title('Model accuracy (Fold %d)' % fold)\n",
    "  else:\n",
    "    axs[0].set_title('Model accuracy')\n",
    "  axs[0].set_ylabel('Accuracy')\n",
    "  axs[0].set_xlabel('Epoch')\n",
    "  axs[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "  axs[0].grid(linestyle=':')\n",
    "\n",
    "  axs[1].plot(history.history['loss'])\n",
    "  axs[1].plot(history.history['val_loss'])\n",
    "  if isinstance(fold, int):\n",
    "    axs[1].set_title('Model loss (Fold %d)' % fold)\n",
    "  else:\n",
    "    axs[1].set_title('Model loss')\n",
    "  axs[1].set_ylabel('Loss')\n",
    "  axs[1].set_xlabel('Epoch')\n",
    "  axs[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "  axs[1].grid(linestyle=':')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "oU_jGa0jHeiZ"
   },
   "outputs": [],
   "source": [
    "#@title Plot Sorted Distribution of Fibril / Non-fibril Ratio\n",
    "def plot_ratio(df):\n",
    "  sorted_motifs = df['ratio'].sort_values(ascending=False).index.tolist()\n",
    "  melted_df = df[['fibril_prob', 'non_fibril_prob']].reset_index().melt(id_vars='motifs', value_vars=['fibril_prob', 'non_fibril_prob'], var_name='fibril_state', value_name='value')\n",
    "  melted_df['motifs'] = pd.Categorical(melted_df['motifs'], categories=sorted_motifs, ordered=True)\n",
    "  melted_df = melted_df.sort_values('motifs')\n",
    "  plt.figure(figsize=(15, 7))\n",
    "  sns.barplot(x='motifs', y='value', hue='fibril_state', data=melted_df, palette=['cyan', 'orange'])\n",
    "  plt.grid(linewidth=1, linestyle=':', axis='y')\n",
    "  plt.xticks(rotation=45)\n",
    "  plt.ylabel('Probability', fontsize=16)\n",
    "  plt.xlabel('Motifs', fontsize=16)\n",
    "  plt.title('Probability of Amino Acid Motifs in Fibril and Non-fibril States, Ranked by Fibril/Non-fibril Ratio', fontsize=18)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\alan_/.cache\\torch\\hub\\facebookresearch_esm_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "esm2_model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t6_8M_UR50D\") # esm2_t6_8M_UR50D/esm2_t12_35M_UR50D\n",
    "esm2_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esm2_embedding(sequence, model, alphabet, avg=True):\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    data = [(\"protein\", sequence)] \n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[6], return_contacts=False)\n",
    "    token_representations = results[\"representations\"][6]\n",
    "    sequence_embedding = token_representations[0, 1:len(sequence) + 1].cpu().numpy()\n",
    "\n",
    "    if avg:\n",
    "        sequence_embedding = token_representations[0, 1:len(sequence) + 1].mean(0).cpu().numpy()\n",
    "    return sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(cv_models, metrics, scalers, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = 'trained_models'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    for i, model in enumerate(cv_models):\n",
    "        model.save(f'{save_dir}/model_fold_{i}.h5') \n",
    "    joblib.dump(metrics, f'{save_dir}/metrics.pkl')\n",
    "    joblib.dump(scalers, f'{save_dir}/scalers.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(load_dir=None):\n",
    "    if load_dir is None:\n",
    "        load_dir = 'trained_models'\n",
    "    cv_models = []\n",
    "    for i in range(5):\n",
    "        model = load_model(f'{load_dir}/model_fold_{i}.h5')\n",
    "        cv_models.append(model)\n",
    "    metrics = joblib.load(f'{load_dir}/metrics.pkl')\n",
    "    scalers = joblib.load(f'{load_dir}/scalers.pkl')\n",
    "    return scalers, cv_models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(metrics):\n",
    "    for metric_name, metric_values in metrics.items():\n",
    "        mean_value = np.mean(metric_values) \n",
    "        print(f\"{metric_name} mean: {mean_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p8erDTuyFInS"
   },
   "outputs": [],
   "source": [
    "#@title N Fold Cross Validation MLP\n",
    "def n_fold_cross_validation(df_filtered, n_splits=5, random_state=42, epochs=200, batch_size=10, plot=True, save_recall=False):\n",
    "  kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "  accuracies = []\n",
    "  precisions = []\n",
    "  recalls = []\n",
    "  f1_scores = []\n",
    "  aucs = []\n",
    "  losses = []\n",
    "  models = []\n",
    "  scalers = []\n",
    "\n",
    "  # Add 'isFibril' (Label) column - fibril: 1; non-fibril: 0\n",
    "  data = df_filtered.copy()\n",
    "  data['isFibril'] = data['fibril_state'].apply(lambda x: 1 if x == 'fibril' else 0)\n",
    "  labels = np.array(data['isFibril'])\n",
    "\n",
    "  fold = 0\n",
    "  for train_index, val_index in kfold.split(data, labels):\n",
    "\n",
    "    fold += 1\n",
    "    print(f'Fold {fold}')\n",
    "\n",
    "    df = df_filtered.copy()\n",
    "\n",
    "    labels_train, labels_val = labels[train_index], labels[val_index]\n",
    "\n",
    "    df['train_seq'] = ''\n",
    "    df['test_seq'] = ''\n",
    "    df.loc[train_index, 'train_seq'] = df.loc[train_index, 'sequence']\n",
    "    df.loc[val_index, 'test_seq'] = df.loc[val_index, 'sequence']\n",
    "\n",
    "    sequences_train = df[df['train_seq'] != '']['sequence'].values\n",
    "    sequences_val = df[df['test_seq'] != '']['sequence'].values\n",
    "\n",
    "    esm2_train_embeddings = np.array([get_esm2_embedding(seq, esm2_model, alphabet) for seq in sequences_train])\n",
    "    esm2_val_embeddings = np.array([get_esm2_embedding(seq, esm2_model, alphabet) for seq in sequences_val])\n",
    "    \n",
    "    data_train = esm2_train_embeddings\n",
    "    data_val = esm2_val_embeddings\n",
    "    input_dim = 320\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_train = scaler.fit_transform(data_train)\n",
    "    data_val = scaler.transform(data_val)\n",
    "    scalers.append(scaler)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_dim=input_dim, kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.2), \n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(data_train, labels_train, epochs=epochs, batch_size=batch_size,\n",
    "              validation_data=(data_val, labels_val),\n",
    "              callbacks=[early_stopping],\n",
    "              verbose=0) # verbose = 1 default\n",
    "\n",
    "    models.append(model)\n",
    "    pred_prob = model.predict(data_val).flatten()\n",
    "\n",
    "    if save_recall:\n",
    "      save_path = f'recall/fold_{fold}_fibril_probabilities.xlsx'\n",
    "      os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "      df_results = pd.DataFrame({\n",
    "          'sequence': sequences_val,\n",
    "          'fibril_probability': pred_prob\n",
    "          })\n",
    "      df_results.to_excel(save_path, index=False)\n",
    "\n",
    "    pred_prob_binary = (pred_prob > 0.5).astype(int)\n",
    "\n",
    "    # Calculate Accuracy, F1 Score, and Loss\n",
    "    acc = accuracy_score(labels_val, pred_prob_binary)\n",
    "    precision = precision_score(labels_val, pred_prob_binary)\n",
    "    recall = recall_score(labels_val, pred_prob_binary)\n",
    "    f1s = f1_score(labels_val, pred_prob_binary)\n",
    "    auc = roc_auc_score(labels_val, pred_prob)\n",
    "    loss = log_loss(labels_val, pred_prob)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1s)\n",
    "    aucs.append(auc)\n",
    "    losses.append(loss)\n",
    "    print()\n",
    "\n",
    "    if plot:\n",
    "      plot_training_history(history, fold)\n",
    "\n",
    "  metrics = {\n",
    "    \"Accuracy\": accuracies,\n",
    "    \"Precision\": precisions,\n",
    "    \"Recall\": recalls,\n",
    "    \"F1-Scores\": f1_scores,\n",
    "    \"AUC\": aucs,\n",
    "    \"Loss\": losses,\n",
    "  }\n",
    "\n",
    "  print_stats(metrics)\n",
    "\n",
    "  return scalers, models, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalers, models, metrics = n_fold_cross_validation(df_filtered, epochs=500, batch_size=16, random_state=91, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers, cv_models, metrics = load_results(load_dir='trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean: 0.9438\n",
      "Precision mean: 0.9100\n",
      "Recall mean: 0.9100\n",
      "F1-Scores mean: 0.9056\n",
      "AUC mean: 0.9390\n",
      "Loss mean: 0.2821\n"
     ]
    }
   ],
   "source": [
    "print_stats(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nae0AjqXXhKw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequence\n",
      "0  FFFFKEKE\n",
      "1  FFFFKEEK\n",
      "2  FFFFEKKE\n",
      "3  FFFKFEKE\n",
      "4  FFFKKFEE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(348, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seq_file_path = \"data/420seq.csv\"\n",
    "\n",
    "used_sequences = df_filtered['sequence'].tolist()\n",
    "\n",
    "all_seq_df = pd.read_csv(new_seq_file_path, header=None)\n",
    "column_names = ['sequence']\n",
    "all_seq_df.columns = column_names\n",
    "all_sequences = all_seq_df[column_names[0]].to_list()\n",
    "filtered_all_seq_df = all_seq_df[~all_seq_df['sequence'].isin(used_sequences)].reset_index(drop=True)\n",
    "\n",
    "print(filtered_all_seq_df.head())\n",
    "filtered_all_seq_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibril_prediction(seq_df, scalers, cv_models, pred_dir=None):\n",
    "    if pred_dir is None:\n",
    "        pred_dir = 'pred_fibril_probs'\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    \n",
    "    index = 1\n",
    "    all_probabilities_df = pd.DataFrame()\n",
    "\n",
    "    for model, scaler in zip(cv_models, scalers):\n",
    "        \n",
    "        features = np.array([get_esm2_embedding(seq, esm2_model, alphabet) for seq in seq_df['sequence']])\n",
    "        features_scaled = scaler.transform(features)\n",
    "\n",
    "        predictions = model.predict(features_scaled)\n",
    "        probabilities = [prob[0] for prob in predictions]\n",
    "        predicted_df = pd.DataFrame({'sequence': seq_df['sequence'].to_list(), f'fibril_probability_model_{index}': probabilities})\n",
    "        \n",
    "        if all_probabilities_df.empty:\n",
    "            all_probabilities_df = predicted_df\n",
    "        else:\n",
    "            all_probabilities_df = all_probabilities_df.merge(predicted_df, on='sequence')\n",
    "        predicted_df.to_csv(f'{pred_dir}/predicted_fibril_state_{index}.csv')\n",
    "        index += 1\n",
    "\n",
    "    all_probabilities_df['average_fibril_probability'] = all_probabilities_df.filter(like='fibril_probability_').mean(axis=1)\n",
    "\n",
    "    sorted_df = all_probabilities_df.sort_values(by='average_fibril_probability', ascending=False)\n",
    "\n",
    "    sorted_df[['sequence', 'average_fibril_probability']].to_csv(f'{pred_dir}/average_fibril_probability.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 999us/step\n"
     ]
    }
   ],
   "source": [
    "fibril_prediction(filtered_all_seq_df, scalers, cv_models, pred_dir='all_seq_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "test_seq_dict = {\n",
    "    \"sequence\": [\n",
    "        \"EFKFFKFE\", \"FKFFKFEE\", \"KFFKFEFE\", \"EKFFKFEF\", \n",
    "        \"FEFEFKFK\", \"FKFEKFEF\", \"KFEKFEFF\", \"EEKFFKFF\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_test_seq = pd.DataFrame(test_seq_dict)\n",
    "fibril_prediction(df_test_seq, scalers, cv_models, pred_dir='test_seq_pred')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
